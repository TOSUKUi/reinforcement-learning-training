実装に向けて必要なもの

Q-table (or Q関数)

- state
  - エージェントが置かれる状態
- action
  - エージェントが行う行動
- reward
  - エージェントがある状態において行った行動における報酬
- transition probability function
  - エージェントが次の状態$s_t$になる確率
- Policy (方策)
  - エージェントがある状態でどんな行動をどのくらいの確率で行うかを表す関数 

- 行動価値関数 
  - $Q^\pi(s, a) = E[G_t|s_t = s, a_t = a, \pi]$
  - sという状態でaという行動を取った時の行動価値。
  - 方策πでえられる報酬の期待値

- $G_t := \sum_{i=0}^{\inf}\gamma^ir_{t+1+i}  (\gamma \in [0, 1])$  

- Q関数:最適行動価値関数
  - 報酬の指標となる関数。この関数による


- Q-learning
  - 環境の情報がない中で学習する方法
  - 最適行動価値関数を求めるための手法
  - $Q(s, a) \eqsim Q(s, a) + \alpha(R(s,a ) + \gamma Q(s', a') - Q(s,a))$
  - $Q(s,a)_{step+1} \eqsim Q(s,a)_{step} + \alpha(R(s,a) + \gamma max_{a' \in A(s')} Q(s', a') - Q(s,a))$

  - εグリーディー法
    - 現状価値が一番高い選択肢ばかり選択するとQテーブルが全く更新されないので一番最適な行動aの確率に対して

- アルゴリズムの実装
  - Q関数の初期化
  - 繰り返し
    - sを初期化
    - 繰り返し
        - sにおいてεグリーディーに乗っ取って行動aを選択
        - sにおける可能な最適解aを行い、その時の報酬を観測
        - 報酬に基づいてQ関数を更新
          - 終端状態の場合$\max_{a' \in A(s')} Q(s', a')$は0
        - $s \leftarrow s'$ 
        - sが終端状態なら繰り返し終了

        